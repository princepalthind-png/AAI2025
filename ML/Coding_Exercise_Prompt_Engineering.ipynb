{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0zEsVFET3w97rRMMfpcA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanRathaur1/AAI2025/blob/main/Coding_Exercise_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvqkn-0g4ALA",
        "outputId": "eee6a00d-387a-4b3a-810c-e939ce77f695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INTENT ===\n",
            "{\n",
            "  \"category\": \"technical\",\n",
            "  \"urgency\": \"high\",\n",
            "  \"summary\": \"Customerâ€™s Wi-Fi drops about every 20 minutes since yesterday.\"\n",
            "}\n",
            "\n",
            "=== PLAN ===\n",
            "1) Power-cycle modem and router (unplug 30s, replug).\n",
            "2) Reseat coax/ethernet cables; check for damage.\n",
            "3) NimbusNet portal â†’ Devices â†’ Router â†’ run line test.\n",
            "4) Temporarily disable QoS/traffic shaping to isolate config issues.\n",
            "5) If drops persist, schedule a technician to check signal levels.\n",
            "\n",
            "=== REPLY ===\n",
            "Hi! Sorry about the dropsâ€”letâ€™s stabilize this fast.\n",
            "\n",
            "Try these steps:\n",
            "1) Power-cycle your modem and router (unplug 30s, then replug).\n",
            "2) Reseat all cables (coax/ethernet) and ensure they click in.\n",
            "3) In the NimbusNet portal â†’ Devices â†’ Router â†’ run a line test.\n",
            "4) If you use QoS, toggle it off briefly to test behavior.\n",
            "5) If it still drops, I can book a tech visit right away.\n",
            "\n",
            "Reply here with results and Iâ€™ll take the next step.\n",
            "\n",
            "=== CLOSING ===\n",
            "Glad we could helpâ€”wishing you smooth streaming! ðŸ™Œ\n"
          ]
        }
      ],
      "source": [
        "# @title Customer Support Prompt Chain (Colab-ready)\n",
        "# @markdown - Runs in **mock mode** by default (no API key needed).\n",
        "# @markdown - Set `USE_MOCK = False` and provide `OPENAI_API_KEY` to use a real model.\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "USE_MOCK = True  # <- set to False to call a real LLM (OpenAI example below)\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "SYSTEM = \"You are a helpful, concise customer support assistant for a company called 'NimbusNet'.\"\n",
        "\n",
        "# ===================== LLM HELPER =====================\n",
        "def call_llm(messages, model=OPENAI_MODEL, temperature=0.2) -> str:\n",
        "    \"\"\"\n",
        "    Replace with your provider if desired. In mock mode, returns canned outputs.\n",
        "    \"\"\"\n",
        "    if USE_MOCK:\n",
        "        return _mock_llm(messages)\n",
        "\n",
        "    # ---- Real OpenAI call (optional) ----\n",
        "    # pip install openai\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Install `openai` first: pip install openai\") from e\n",
        "\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Please set OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "# ===================== MOCK (NO API NEEDED) =====================\n",
        "def _mock_llm(messages) -> str:\n",
        "    \"\"\"\n",
        "    Minimal mock so this runs in Colab without internet or keys.\n",
        "    Looks at the last user prompt to return plausible outputs.\n",
        "    \"\"\"\n",
        "    last = [m for m in messages if m[\"role\"] == \"user\"][-1][\"content\"]\n",
        "\n",
        "    if \"Classify the user's problem\" in last:\n",
        "        return json.dumps({\n",
        "            \"category\": \"technical\",\n",
        "            \"urgency\": \"high\",\n",
        "            \"summary\": \"Customerâ€™s Wi-Fi drops about every 20 minutes since yesterday.\"\n",
        "        })\n",
        "    if \"Draft a step-by-step troubleshooting plan\" in last:\n",
        "        return (\n",
        "            \"1) Power-cycle modem and router (unplug 30s, replug).\\n\"\n",
        "            \"2) Reseat coax/ethernet cables; check for damage.\\n\"\n",
        "            \"3) NimbusNet portal â†’ Devices â†’ Router â†’ run line test.\\n\"\n",
        "            \"4) Temporarily disable QoS/traffic shaping to isolate config issues.\\n\"\n",
        "            \"5) If drops persist, schedule a technician to check signal levels.\"\n",
        "        )\n",
        "    if \"Write a friendly response to the customer\" in last:\n",
        "        return (\n",
        "            \"Hi! Sorry about the dropsâ€”letâ€™s stabilize this fast.\\n\\n\"\n",
        "            \"Try these steps:\\n\"\n",
        "            \"1) Power-cycle your modem and router (unplug 30s, then replug).\\n\"\n",
        "            \"2) Reseat all cables (coax/ethernet) and ensure they click in.\\n\"\n",
        "            \"3) In the NimbusNet portal â†’ Devices â†’ Router â†’ run a line test.\\n\"\n",
        "            \"4) If you use QoS, toggle it off briefly to test behavior.\\n\"\n",
        "            \"5) If it still drops, I can book a tech visit right away.\\n\\n\"\n",
        "            \"Reply here with results and Iâ€™ll take the next step.\"\n",
        "        )\n",
        "    if \"short closing line\" in last or \"closing line\" in last:\n",
        "        return \"Glad we could helpâ€”wishing you smooth streaming! ðŸ™Œ\"\n",
        "    return \"OK.\"\n",
        "\n",
        "# ===================== CHAIN STEPS =====================\n",
        "def detect_intent(user_issue: str) -> dict:\n",
        "    \"\"\"\n",
        "    Step 1: Identify intent and summarize.\n",
        "    Returns dict: {category, urgency, summary}\n",
        "    \"\"\"\n",
        "    msg = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Classify the user's problem and extract key fields.\n",
        "\n",
        "User issue: \\\"\\\"\\\"{user_issue}\\\"\\\"\\\"\n",
        "\n",
        "Return JSON with:\n",
        "- \"category\": one of [\"billing\",\"technical\",\"account\",\"shipping\",\"other\"]\n",
        "- \"urgency\": one of [\"low\",\"medium\",\"high\"]\n",
        "- \"summary\": 1-sentence summary\n",
        "Only return JSON.\n",
        "\"\"\"}]\n",
        "    out = call_llm(msg)\n",
        "    # Strip possible code fences and parse JSON robustly\n",
        "    out = re.sub(r\"```(?:json)?|```\", \"\", out).strip()\n",
        "    try:\n",
        "        data = json.loads(out)\n",
        "    except json.JSONDecodeError:\n",
        "        data = {\"category\": \"other\", \"urgency\": \"medium\", \"summary\": user_issue[:120]}\n",
        "    return data\n",
        "\n",
        "def troubleshoot(state: dict) -> str:\n",
        "    \"\"\"\n",
        "    Step 2: Generate a numbered troubleshooting plan based on detected state.\n",
        "    \"\"\"\n",
        "    msg = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Given this classified issue:\n",
        "{json.dumps(state, ensure_ascii=False)}\n",
        "\n",
        "Draft a step-by-step troubleshooting plan (3â€“6 bullets) tailored to the issue and clearly numbered.\n",
        "Avoid generic advice; reference NimbusNet specifics (router reset, plan limits, portal steps).\n",
        "\"\"\"}]\n",
        "    return call_llm(msg)\n",
        "\n",
        "def craft_response(user_issue: str, plan: str, max_words: int = 140) -> str:\n",
        "    \"\"\"\n",
        "    Step 3: Friendly customer response using the plan.\n",
        "    \"\"\"\n",
        "    msg = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Write a friendly response to the customer:\n",
        "- Start with a 1-sentence acknowledgement referencing their issue: \\\"{user_issue}\\\"\n",
        "- Include the numbered troubleshooting plan below under \"Try these steps:\"\n",
        "- Offer a fallback/next-step if it fails (e.g., escalate ticket, schedule tech visit)\n",
        "- Keep under {max_words} words.\n",
        "Troubleshooting plan:\n",
        "{plan}\n",
        "\"\"\"}]\n",
        "    return call_llm(msg, temperature=0.4)\n",
        "\n",
        "def closing_message() -> str:\n",
        "    \"\"\"\n",
        "    Step 4: Short, warm closing line.\n",
        "    \"\"\"\n",
        "    msg = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short closing line (<=20 words) that sounds warm and professional.\"}\n",
        "    ]\n",
        "    return call_llm(msg, temperature=0.7)\n",
        "\n",
        "# ===================== ORCHESTRATOR =====================\n",
        "@dataclass\n",
        "class ChainResult:\n",
        "    intent: dict\n",
        "    plan: str\n",
        "    reply: str\n",
        "    closing: str\n",
        "\n",
        "def run_chain(user_issue: str) -> ChainResult:\n",
        "    intent = detect_intent(user_issue)\n",
        "    plan = troubleshoot(intent)\n",
        "    reply = craft_response(user_issue, plan)\n",
        "    closing = closing_message()\n",
        "    return ChainResult(intent=intent, plan=plan, reply=reply, closing=closing)\n",
        "\n",
        "# ===================== DEMO =====================\n",
        "# You can change this input and re-run the cell in Colab.\n",
        "example_issue = \"My internet drops every 20 minutes since yesterday, and Iâ€™m on a deadline.\"\n",
        "result = run_chain(example_issue)\n",
        "\n",
        "print(\"=== INTENT ===\")\n",
        "print(json.dumps(result.intent, indent=2, ensure_ascii=False))\n",
        "print(\"\\n=== PLAN ===\")\n",
        "print(result.plan)\n",
        "print(\"\\n=== REPLY ===\")\n",
        "print(result.reply)\n",
        "print(\"\\n=== CLOSING ===\")\n",
        "print(result.closing)\n",
        "\n",
        "# ===================== HOW TO USE REAL MODEL =====================\n",
        "# 1) Set USE_MOCK = False near the top\n",
        "# 2) In Colab, run:\n",
        "#    %pip install openai\n",
        "#    import os; os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "# 3) Re-run this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ReACT Code Generation Demo (Colab-ready)\n",
        "# @markdown - Works in **mock mode** by default (no API key needed).\n",
        "# @markdown - To use a real model: set `USE_MOCK = False`, then `pip install openai` and set `OPENAI_API_KEY`.\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, re, sys, ast\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ======================= CONFIG =======================\n",
        "USE_MOCK = True          # <- set to False to call OpenAI\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "MAX_TOOL_LOOPS = 4\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a careful assistant that uses a strict ReACT workflow.\n",
        "\n",
        "Rules:\n",
        "- Think step-by-step in lines that start with 'Thought:' (plain text).\n",
        "- When you need to run Python, write exactly:\n",
        "  Action: python\n",
        "  <<<CODE>>>\n",
        "  # Python code with no imports\n",
        "  # Use only builtins like range, len, sum, min, max, print\n",
        "  # If you produce a value, store it in a variable named 'result'\n",
        "  <<<ENDCODE>>>\n",
        "- Wait for an Observation from the tool, then continue with another Thought.\n",
        "- Finish with a single line starting with 'Final:' that states your concise final answer.\n",
        "\n",
        "Constraints:\n",
        "- Do NOT import modules (no 'import', 'from', etc.).\n",
        "- Keep code minimal and deterministic.\n",
        "\"\"\"\n",
        "\n",
        "USER_TEMPLATE = \"\"\"You can use one tool:\n",
        "- python: executes the code you provide between <<<CODE>>> and <<<ENDCODE>>> and returns stdout, and the value of 'result' if present.\n",
        "\n",
        "Task:\n",
        "{task}\n",
        "\n",
        "Follow the exact format:\n",
        "\n",
        "Thought: <your reasoning>\n",
        "Action: python\n",
        "<<<CODE>>>\n",
        "# your python code here (no imports)\n",
        "<<<ENDCODE>>>\n",
        "Observation: <tool output>\n",
        "Thought: <what it means / next step>\n",
        "... (repeat if needed)\n",
        "Final: <concise final answer>\"\"\"\n",
        "\n",
        "# ======================= LLM HELPER =======================\n",
        "def call_llm(messages, model=OPENAI_MODEL, temperature=0.2) -> str:\n",
        "    \"\"\"Return assistant text from either a real provider or a mock.\"\"\"\n",
        "    if USE_MOCK:\n",
        "        return _mock_llm(messages)\n",
        "\n",
        "    # ---- Real OpenAI call (optional) ----\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Install `openai` first: %pip install openai\") from e\n",
        "\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Please set OPENAI_API_KEY in the environment.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def _mock_llm(messages) -> str:\n",
        "    \"\"\"Simple deterministic mock that follows the ReACT format for demo purposes.\"\"\"\n",
        "    # We will parse whether an Observation has been provided; if so, produce Final.\n",
        "    transcript = \"\\n\".join(m[\"content\"] for m in messages if m[\"role\"] in (\"user\",\"assistant\"))\n",
        "    if \"Observation:\" not in transcript:\n",
        "        # First model turn: reason + produce code\n",
        "        return (\n",
        "            \"Thought: I will compute the mean and population standard deviation step-by-step.\\n\"\n",
        "            \"Action: python\\n\"\n",
        "            \"<<<CODE>>>\\n\"\n",
        "            \"data = [12, 15, 14, 10, 19]\\n\"\n",
        "            \"n = len(data)\\n\"\n",
        "            \"mean = sum(data)/n\\n\"\n",
        "            \"var = sum((x-mean)**2 for x in data)/n  # population variance\\n\"\n",
        "            \"std = var ** 0.5\\n\"\n",
        "            \"print(f\\\"mean={mean}, std={std}\\\")\\n\"\n",
        "            \"result = (mean, std)\\n\"\n",
        "            \"<<<ENDCODE>>>\\n\"\n",
        "        )\n",
        "    else:\n",
        "        # Second model turn: read the observation and finalize\n",
        "        m = re.search(r\"Observation:\\s*(.*)\", transcript, re.S)\n",
        "        obs = m.group(1).strip() if m else \"\"\n",
        "        return (\n",
        "            f\"Thought: The observation already contains computed statistics ({obs}). \"\n",
        "            \"No further code is needed.\\n\"\n",
        "            \"Final: Computed mean and population std from the list.\"\n",
        "        )\n",
        "\n",
        "# ======================= PYTHON TOOL =======================\n",
        "def run_python_safely(code: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute untrusted Python with a tiny allowlist of builtins.\n",
        "    - No imports allowed (we pre-scan AST to reject Import/ImportFrom).\n",
        "    - Captures stdout; returns stdout + 'result=' value if present.\n",
        "    \"\"\"\n",
        "    # Parse and reject imports or other disallowed nodes\n",
        "    tree = ast.parse(code)\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
        "            raise RuntimeError(\"Imports are not allowed.\")\n",
        "        # Optional: forbid exec/eval/etc. (we already restrict builtins)\n",
        "        if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n",
        "            if node.func.id in {\"__import__\", \"eval\", \"exec\", \"open\", \"compile\"}:\n",
        "                raise RuntimeError(f\"Call to {node.func.id} is not allowed.\")\n",
        "\n",
        "    # Minimal safe builtins\n",
        "    allowed_builtins = {\n",
        "        \"range\": range, \"len\": len, \"sum\": sum, \"min\": min, \"max\": max, \"print\": print,\n",
        "        \"abs\": abs, \"round\": round, \"enumerate\": enumerate\n",
        "    }\n",
        "    g = {\"__builtins__\": allowed_builtins}\n",
        "    l = {}\n",
        "\n",
        "    from io import StringIO\n",
        "    buf = StringIO()\n",
        "    old = sys.stdout\n",
        "    try:\n",
        "        sys.stdout = buf\n",
        "        exec(code, g, l)\n",
        "    finally:\n",
        "        sys.stdout = old\n",
        "\n",
        "    stdout = buf.getvalue().strip()\n",
        "    if \"result\" in l:\n",
        "        if stdout:\n",
        "            return f\"{stdout}\\nresult={l['result']!r}\"\n",
        "        return f\"result={l['result']!r}\"\n",
        "    return stdout or \"<no output>\"\n",
        "\n",
        "# ======================= REACT LOOP =======================\n",
        "@dataclass\n",
        "class ReactResult:\n",
        "    final: str\n",
        "    transcript: str\n",
        "\n",
        "def react(task: str, max_loops: int = MAX_TOOL_LOOPS) -> ReactResult:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(task=task)}\n",
        "    ]\n",
        "    transcript = []\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        resp = call_llm(messages)\n",
        "        transcript.append(f\"[ASSISTANT]\\n{resp}\")\n",
        "        messages.append({\"role\": \"assistant\", \"content\": resp})\n",
        "\n",
        "        # Parse for Action/Code/Final\n",
        "        final_match = re.search(r\"\\bFinal:\\s*(.*)\", resp, re.S)\n",
        "        if final_match:\n",
        "            return ReactResult(final=final_match.group(1).strip(), transcript=\"\\n\".join(transcript))\n",
        "\n",
        "        action_match = re.search(r\"Action:\\s*python\", resp)\n",
        "        code_match = re.search(r\"<<<CODE>>>(.*?)<<<ENDCODE>>>\", resp, re.S)\n",
        "\n",
        "        if action_match and code_match:\n",
        "            code = code_match.group(1).strip()\n",
        "            try:\n",
        "                observation = run_python_safely(code)\n",
        "            except Exception as e:\n",
        "                observation = f\"[tool-error] {type(e).__name__}: {e}\"\n",
        "            obs_text = f\"Observation: {observation}\"\n",
        "            transcript.append(f\"[TOOL]\\n{obs_text}\")\n",
        "            messages.append({\"role\": \"user\", \"content\": obs_text})\n",
        "            continue\n",
        "\n",
        "        # If neither code nor final, stop to avoid hanging\n",
        "        break\n",
        "\n",
        "    return ReactResult(final=\"(Stopped without Final.)\", transcript=\"\\n\".join(transcript))\n",
        "\n",
        "# ======================= DEMO =======================\n",
        "# You can change TASK and re-run this cell.\n",
        "TASK = \"Generate Python to compute the mean and population standard deviation of [12, 15, 14, 10, 19] and return them.\"\n",
        "\n",
        "result = react(TASK)\n",
        "\n",
        "print(\"=== FINAL ===\")\n",
        "print(result.final)\n",
        "print(\"\\n=== TRACE ===\")\n",
        "print(result.transcript)\n",
        "\n",
        "# ======================= HOW TO USE REAL MODEL =======================\n",
        "# 1) Set USE_MOCK = False at the top of this cell.\n",
        "# 2) In Colab, run:\n",
        "#    %pip install openai\n",
        "#    import os; os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "# 3) Re-run the cell.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63kRUIkN7f_w",
        "outputId": "7384fe12-f6bd-419a-a410-495fdfb143aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FINAL ===\n",
            "<concise final answer>). No further code is needed.\n",
            "Final: Computed mean and population std from the list.\n",
            "\n",
            "=== TRACE ===\n",
            "[ASSISTANT]\n",
            "Thought: The observation already contains computed statistics (<tool output>\n",
            "Thought: <what it means / next step>\n",
            "... (repeat if needed)\n",
            "Final: <concise final answer>). No further code is needed.\n",
            "Final: Computed mean and population std from the list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3_self_reflection.py\n",
        "from llm_helper import call_llm\n",
        "\n",
        "SYSTEM = \"You are a precise editor. Be concise, specific, and constructive.\"\n",
        "\n",
        "CRITIC_PROMPT = \"\"\"You're evaluating the following summary for clarity, accuracy, completeness, and concision.\n",
        "Return JSON with keys:\n",
        "- strengths: 1-3 short bullets\n",
        "- issues: 2-5 concrete issues (quote problematic text when helpful)\n",
        "- fix_plan: 3-6 bullet plan of improvements (specific edits, not generic advice)\n",
        "Summary:\n",
        "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
        "Only return JSON.\n",
        "\"\"\"\n",
        "\n",
        "REWRITE_PROMPT = \"\"\"Improve the summary using the critique below.\n",
        "Rules:\n",
        "- Keep it 90â€“140 words.\n",
        "- Fix inaccuracies, tighten wording, improve structure.\n",
        "- Preserve all critical facts.\n",
        "Critique:\n",
        "{critique}\n",
        "Original Summary:\n",
        "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
        "Return only the improved summary (no preface).\n",
        "\"\"\"\n",
        "\n",
        "def self_refine(summary: str) -> dict:\n",
        "    # Step 1: Critique\n",
        "    critique = call_llm(\n",
        "        [{\"role\":\"system\",\"content\":SYSTEM},\n",
        "         {\"role\":\"user\",\"content\":CRITIC_PROMPT.format(summary=summary)}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "    # Step 2: Improve\n",
        "    improved = call_llm(\n",
        "        [{\"role\":\"system\",\"content\":SYSTEM},\n",
        "         {\"role\":\"user\",\"content\":REWRITE_PROMPT.format(critique=critique, summary=summary)}],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return {\"critique\": critique, \"improved\": improved}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    draft = (\"NimbusNetâ€™s new plan expands speeds but some users see unstable Wi-Fi and billing confusion. \"\n",
        "             \"Support should remind customers to reboot. The rollout started last month across regions.\")\n",
        "    out = self_refine(draft)\n",
        "    print(\"=== CRITIQUE ===\\n\", out[\"critique\"])\n",
        "    print(\"\\n=== IMPROVED SUMMARY ===\\n\", out[\"improved\"])\n"
      ],
      "metadata": {
        "id": "FagKragI8LdA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
